# ADV<sup>2</sup>
Adversarial Attacks against Interpretable Deep Learning Systems (Presented in 2020 the 29th USENIX Security Symposium)

## Requirements

Pytorch

## Folder Structure

exp_attacks_fixed: main attack files for CAM, RTS

rev1: MASK attack implementation, attack for ISIC dataset

rev2: GRAD attack implementation, attack for CIFAR10 dataset

expr_detect: detecting adversarial examples

expr_shape: random shape attack

expr_transfer: measure transferability of our attack

Notice: the code name of our project during the development is ACID instead of ADV<sup>2</sup>. 


## Bibtex:
Please cite our paper, if you happen to use this codebase:

```
@inproceedings{zhang:2020:adv2,
  title={Interpretable deep learning under fire},
  author={Zhang, Xinyang and Wang, Ningfei and Shen, Hua and Ji, Shouling and Luo, Xiapu and Wang, Ting},
  booktitle={29th $\{$USENIX$\}$ Security Symposium ($\{$USENIX$\}$ Security 20)},
  year={2020}
}
```# adv2
